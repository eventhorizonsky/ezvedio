{
  "id": "91f6bbe2-ed41-4fd6-bac7-71d5b5864ecb",
  "revision": 0,
  "last_node_id": 78,
  "last_link_id": 135,
  "nodes": [
    {
      "id": 39,
      "type": "VAELoader",
      "pos": [
        20,
        340
      ],
      "size": [
        330,
        60
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "slot_index": 0,
          "links": [
            76
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "VAELoader",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "models": [
          {
            "name": "qwen_image_vae.safetensors",
            "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors",
            "directory": "vae"
          }
        ],
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        "qwen_image_vae.safetensors"
      ]
    },
    {
      "id": 8,
      "type": "VAEDecode",
      "pos": [
        1170,
        -90
      ],
      "size": [
        210,
        46
      ],
      "flags": {
        "collapsed": false
      },
      "order": 11,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 128
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 76
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "slot_index": 0,
          "links": [
            110
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "VAEDecode",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": []
    },
    {
      "id": 76,
      "type": "CLIPLoaderGGUF",
      "pos": [
        47.02664552750114,
        194.4721607297193
      ],
      "size": [
        270,
        82
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            131,
            132
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPLoaderGGUF"
      },
      "widgets_values": [
        "Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf",
        "qwen_image"
      ]
    },
    {
      "id": 75,
      "type": "UnetLoaderGGUF",
      "pos": [
        43.52329963836817,
        66.0161447948409
      ],
      "size": [
        270,
        58
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            133
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "UnetLoaderGGUF"
      },
      "widgets_values": [
        "qwen-image-2512-Q4_K_M.gguf"
      ]
    },
    {
      "id": 58,
      "type": "EmptySD3LatentImage",
      "pos": [
        50,
        510
      ],
      "size": [
        270,
        106
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            107
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "EmptySD3LatentImage",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        1024,
        1024,
        1
      ]
    },
    {
      "id": 71,
      "type": "Note",
      "pos": [
        413.2495458214134,
        -187.7313538565722
      ],
      "size": [
        300,
        88
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "Increase the shift if you get too many blury/dark/bad images. Decrease if you want to try increasing detail."
      ],
      "color": "#432",
      "bgcolor": "#000"
    },
    {
      "id": 7,
      "type": "CLIPTextEncode",
      "pos": [
        390,
        440
      ],
      "size": [
        425.27801513671875,
        180.6060791015625
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 132
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            52
          ]
        }
      ],
      "title": "CLIP Text Encode (Negative Prompt)",
      "properties": {
        "Node name for S&R": "CLIPTextEncode",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        ""
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 60,
      "type": "SaveImage",
      "pos": [
        1214.8625427638335,
        228.11312988896637
      ],
      "size": [
        490,
        600
      ],
      "flags": {},
      "order": 12,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 110
        }
      ],
      "outputs": [],
      "properties": {
        "Node name for S&R": "SaveImage",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        "ComfyUI"
      ]
    },
    {
      "id": 3,
      "type": "KSampler",
      "pos": [
        871.7530216461431,
        75.17559175946211
      ],
      "size": [
        300,
        474
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 134
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 46
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 52
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 107
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "slot_index": 0,
          "links": [
            128
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "KSampler",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        638623049701463,
        "randomize",
        40,
        4,
        "euler",
        "simple",
        1
      ]
    },
    {
      "id": 66,
      "type": "ModelSamplingAuraFlow",
      "pos": [
        405.0750720801024,
        -46.05353422612876
      ],
      "size": [
        300,
        58
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 133
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            135
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "ModelSamplingAuraFlow",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        3
      ]
    },
    {
      "id": 77,
      "type": "CFGNorm",
      "pos": [
        408.6887194823357,
        68.5969429331689
      ],
      "size": [
        289.78222645397136,
        73.54317792812026
      ],
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 135
        }
      ],
      "outputs": [
        {
          "name": "patched_model",
          "type": "MODEL",
          "links": [
            134
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "CFGNorm"
      },
      "widgets_values": [
        1
      ]
    },
    {
      "id": 6,
      "type": "CLIPTextEncode",
      "pos": [
        390,
        240
      ],
      "size": [
        422.84503173828125,
        164.31304931640625
      ],
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 131
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            46
          ]
        }
      ],
      "title": "CLIP Text Encode (Positive Prompt)",
      "properties": {
        "Node name for S&R": "CLIPTextEncode",
        "cnr_id": "comfy-core",
        "ver": "0.3.48",
        "enableTabs": false,
        "tabWidth": 65,
        "tabXOffset": 10,
        "hasSecondTab": false,
        "secondTabText": "Send Back",
        "secondTabOffset": 80,
        "secondTabWidth": 65,
        "widget_ue_connectable": {}
      },
      "widgets_values": [
        "A 28-year-old woman with warm brown eyes and a relaxed, natural expression. Her curly auburn hair is a bit messy, tucked behind one ear. A few freckles across her nose, no visible makeup. She wears a worn-in denim jacket over a faded band t-shirt. She stands at an outdoor farmer's market, wooden crates of vegetables and handwritten price signs in the soft background blur. Natural overcast daylight, no flash."
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 78,
      "type": "Note",
      "pos": [
        -216.28658876090321,
        452.35998374927476
      ],
      "size": [
        210,
        193.18505033067714
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "title": "Image Size",
      "properties": {},
      "widgets_values": [
        "Qwen Image was trained natively on image size up to 1328x1328. You can increase image size to get better resolution but it will increase generation time. "
      ],
      "color": "#432",
      "bgcolor": "#653"
    }
  ],
  "links": [
    [
      46,
      6,
      0,
      3,
      1,
      "CONDITIONING"
    ],
    [
      52,
      7,
      0,
      3,
      2,
      "CONDITIONING"
    ],
    [
      76,
      39,
      0,
      8,
      1,
      "VAE"
    ],
    [
      107,
      58,
      0,
      3,
      3,
      "LATENT"
    ],
    [
      110,
      8,
      0,
      60,
      0,
      "IMAGE"
    ],
    [
      128,
      3,
      0,
      8,
      0,
      "LATENT"
    ],
    [
      131,
      76,
      0,
      6,
      0,
      "CLIP"
    ],
    [
      132,
      76,
      0,
      7,
      0,
      "CLIP"
    ],
    [
      133,
      75,
      0,
      66,
      0,
      "MODEL"
    ],
    [
      134,
      77,
      0,
      3,
      0,
      "MODEL"
    ],
    [
      135,
      66,
      0,
      77,
      0,
      "MODEL"
    ]
  ],
  "groups": [
    {
      "id": 1,
      "title": "Step1 - Load models",
      "bounding": [
        10,
        -20,
        350,
        433.6000061035156
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 2,
      "title": "Step2 - Image size",
      "bounding": [
        10,
        430,
        350,
        210
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    },
    {
      "id": 3,
      "title": "Step3 - Prompt",
      "bounding": [
        380,
        160,
        450,
        470
      ],
      "color": "#3f789e",
      "font_size": 24,
      "flags": {}
    }
  ],
  "config": {},
  "extra": {
    "ds": {
      "scale": 1.3791187490184296,
      "offset": [
        -450.9099255747193,
        -165.21504486576475
      ]
    },
    "frontendVersion": "1.35.9",
    "workflowRendererVersion": "LG",
    "ue_links": [],
    "links_added_by_ue": [],
    "VHS_latentpreview": false,
    "VHS_latentpreviewrate": 0,
    "VHS_MetadataImage": true,
    "VHS_KeepIntermediate": true
  },
  "version": 0.4
}